{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FreNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Oc7c4L0DWyU",
        "outputId": "c08f26de-03b2-4455-fa36-3762f1bc88ad"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.fftpack import dct\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "'''\n",
        "Usage:\n",
        "as image preprocessor by calling DCT() as a transform\n",
        "\n",
        "input: 128*128*3 PIL image\n",
        "output: 64*256 torch array (histogram)\n",
        "\n",
        "[128*128] => (crop) => [256 * [8*8]] => (DCT_2d) => [256 * [8 * 8]] => reshape => [256 * 64]  \n",
        "'''\n",
        "class DCT(object):\n",
        "    def __init__(self):\n",
        "        self.BLOCK_HEIGHT = 8\n",
        "        self.BLOCK_WIDTH = 8\n",
        "        self.BLOCK_SIZE = (self.BLOCK_HEIGHT, self.BLOCK_WIDTH)\n",
        "\n",
        "    def div_block(self, img, block_size):\n",
        "        img_height = img.height\n",
        "        img_width = img.width\n",
        "        block_height = block_size[0]\n",
        "        block_width = block_size[1]\n",
        "        assert(img_height % block_height == 0)\n",
        "        assert(img_width % block_width == 0)\n",
        "\n",
        "        blocks = []\n",
        "        for i in range(0,img_height,block_height):\n",
        "            for j in range(0,img_width,block_width):\n",
        "                box = (j, i, j+block_width, i+block_height)\n",
        "                block = np.array(img.crop(box))\n",
        "                blocks.append(block)\n",
        "        return np.array(blocks)\n",
        "\n",
        "    def dct2(self, array_2d):\n",
        "        return dct(dct(array_2d.T, norm = 'ortho').T, norm = 'ortho')\n",
        "\n",
        "    def _dct2(self, array_2d):\n",
        "        return dct(dct(array_2d, norm = 'ortho').T, norm = 'ortho').T\n",
        "\n",
        "    def __call__(self, img):\n",
        "        image = img\n",
        "        blocks = self.div_block(image, self.BLOCK_SIZE)\n",
        "        b_blocks, g_blocks, r_blocks = blocks[:, :, :, 0], blocks[:, :, :, 1], blocks[:, :, :, 2]\n",
        "        test_blocks = (b_blocks + g_blocks + r_blocks) / 3 # naive greyscale\n",
        "        result = np.array([self._dct2(test_block) for test_block in test_blocks])\n",
        "        # return a torch.tensor\n",
        "        return torch.from_numpy(result.reshape(256, 64).T).float()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"DCT\"\n",
        "\n",
        "'''\n",
        "Usage: Same as DCT()\n",
        "\n",
        "input: 64*256 torch array (histogram)\n",
        "output: 64*256 torch array (frequency histogram)\n",
        "'''\n",
        "class DFT(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, freq):\n",
        "        # convert into complex form containing real and imaginary part\n",
        "        cmplx = torch.from_numpy(np.zeros((freq.shape[0], freq.shape[1], 2)))\n",
        "        cmplx[:, :, 0] += freq\n",
        "        out = torch.fft(cmplx, 1)[:, :, 0]\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"DFT\"\n",
        "\n",
        "class Ycbcr_convert():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return img.convert('YCbCr')\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Convert a PIL Image from RGB to YCbCr\"\n",
        "\n",
        "train_path='/content/drive/MyDrive/colab_data/train/'\n",
        "test_path='/content/drive/MyDrive/colab_data/test/'\n",
        "\n",
        "image_height = 128\n",
        "image_width  = 128\n",
        "mytransform = transforms.Compose([\n",
        "    transforms.Resize((image_height,image_width), interpolation=Image.BICUBIC),\n",
        "    Ycbcr_convert(),\n",
        "    DCT(),\n",
        "    #DFT()\n",
        "])\n",
        "\n",
        "trainset = datasets.ImageFolder(train_path, transform=mytransform)\n",
        "testset = datasets.ImageFolder(test_path, transform=mytransform)\n",
        "\n",
        "print('Total no. of train images: ', len(trainset))\n",
        "print('Total no. of test images: ', len(testset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no. of train images:  14459\n",
            "Total no. of test images:  3662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKaz2ab8Vgt2",
        "outputId": "5a3f02b7-13eb-4c7b-b8de-db830c073440"
      },
      "source": [
        "#specify train, test images count here. Maintain 80-20 split\n",
        "train_img_count = 400\n",
        "test_img_count = 100\n",
        "\n",
        "#specify epochs count\n",
        "MAX_EPOCH = 10\n",
        "\n",
        "\n",
        "trainset = torch.utils.data.random_split(trainset, [train_img_count, len(trainset)-train_img_count])[0]\n",
        "testset = torch.utils.data.random_split(testset, [test_img_count, len(testset)-test_img_count])[0]\n",
        "\n",
        "\n",
        "print('Total no. of train set images: ', len(trainset))\n",
        "print('Total no. of test set images: ', len(testset))\n",
        "\n",
        "root=pathlib.Path(train_path)\n",
        "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
        "print('classes:', classes)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True) \n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no. of train set images:  400\n",
            "Total no. of test set images:  100\n",
            "classes: ['Boredom', 'Confusion', 'Engagement', 'Frustration']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjk-OnXEelq8"
      },
      "source": [
        "#custom layer/more like activation function\n",
        "def custom_layer1d(X, pool_size):\n",
        "    Y = torch.zeros(X.shape[0] - pool_size + 2, X.shape[1] - pool_size + 2, X.shape[2] - pool_size + 2)\n",
        "    #print(X.shape[0], X.shape[1], X.shape[2])\n",
        "    #print(Y.shape[0], Y.shape[1], Y.shape[2])\n",
        "    for i in range(Y.shape[0]):\n",
        "      for j in range(Y.shape[1]):\n",
        "        for k in range(Y.shape[2]):\n",
        "          Y[i,j,k] = X[i:i + pool_size, j:j + pool_size, k:k + pool_size].max()-X[i:i + pool_size, j:j + pool_size, k:k + pool_size].min().abs()\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6haKTPqVvV2T"
      },
      "source": [
        "class FreNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FreNet, self).__init__()\n",
        "        self.backbone = nn.Sequential(nn.Conv1d(64, 32, 3, padding=1),\n",
        "                             nn.BatchNorm1d(32),\n",
        "                             nn.ReLU(),\n",
        "                             nn.MaxPool1d(2),\n",
        "                             nn.Conv1d(32, 64, 3, padding=1),\n",
        "                             nn.BatchNorm1d(64),\n",
        "                             nn.ReLU(),\n",
        "                             nn.MaxPool1d(2),\n",
        "                             #nn.Conv1d(64, 128, 3, padding=1),\n",
        "                             #nn.BatchNorm1d(128),\n",
        "                             #nn.ReLU(),\n",
        "                             #nn.MaxPool1d(2),\n",
        "                             nn.Flatten(),\n",
        "                             nn.Linear(4096, 64),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(64, 64))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.backbone.forward(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzyDqpTqMpz6"
      },
      "source": [
        "device = 'cpu'\n",
        "model = FreNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
        "\n",
        "# Hyper-parameters\n",
        "learning_rate = 0.0001 # adopt a small lr to ensure convergence\n",
        "batch_size = 32\n",
        "resumetraining = False\n",
        "print_every = 1\n",
        "test_n_savemodel_every_epoch = 1\n",
        "#device = 'cuda'\n",
        "seed_no = 0\n",
        "stop_at_loss = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVeHSqe-4DTL"
      },
      "source": [
        "#Train model \n",
        "#print(checkpoint)\n",
        "for epoch in range(MAX_EPOCH):\n",
        "    total_loss, total_acc = 0, 0\n",
        "    cnt = 0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        X, y = data[0].float().to(device), data[1].to(device)\n",
        "        optimizer.zero_grad() \n",
        "       \n",
        "        # forward\n",
        "        out = model(X)        \n",
        "        #print(out, y_pred)\n",
        "        loss = criterion(out, y)\n",
        "\n",
        "        # backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # stats\n",
        "        y_pred = torch.argmax(out, dim=1)\n",
        "        total_acc += (y_pred == y).sum().item() / len(y_pred)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        cnt += 1\n",
        "        if i % print_every == 0:\n",
        "            avg_loss = total_loss / cnt\n",
        "            avg_acc = total_acc / cnt\n",
        "            total_acc, total_loss = 0, 0\n",
        "            cnt = 0\n",
        "            # print(out.T, '\\n', y_pred.T, '\\n', y.T)\n",
        "            print('[Epoch %d Iter %d] Loss: %5f  Acc: %5f' % (epoch+1, i+1, avg_loss, avg_acc))\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': criterion,\n",
        "            }, \n",
        "            #specify save path along with model name\n",
        "            '/content/drive/MyDrive/colab_data/All_models/model_name')\n",
        "#print(checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "B3Avx7Gr3N32",
        "outputId": "562028a4-7489-4c1a-a0ad-e807c58237ce"
      },
      "source": [
        "#for saving checkpoints \n",
        "\"\"\"# load the model checkpoint\n",
        "checkpoint = torch.load('/content/drive/MyDrive/colab_data/models')\n",
        "#print(checkpoint)\n",
        "# load model weights state_dict\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print('Previously trained model weights state_dict loaded...')\n",
        "# load trained optimizer state_dict\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "print('Previously trained optimizer state_dict loaded...')\n",
        "epoch = checkpoint['epoch']\n",
        "# load the criterion\n",
        "criterion = checkpoint['loss']\n",
        "print('Trained model loss function loaded...')\n",
        "print(f\"Previously trained for {epoch} number of epochs...\")\n",
        "# train for more epochs\n",
        "epoch = 5\n",
        "print(f\"Train for {epoch} more epochs...\")\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# load the model checkpoint\\ncheckpoint = torch.load(\\'/content/drive/MyDrive/colab_data/models\\')\\n#print(checkpoint)\\n# load model weights state_dict\\nmodel.load_state_dict(checkpoint[\\'model_state_dict\\'])\\nprint(\\'Previously trained model weights state_dict loaded...\\')\\n# load trained optimizer state_dict\\noptimizer.load_state_dict(checkpoint[\\'optimizer_state_dict\\'])\\nprint(\\'Previously trained optimizer state_dict loaded...\\')\\nepoch = checkpoint[\\'epoch\\']\\n# load the criterion\\ncriterion = checkpoint[\\'loss\\']\\nprint(\\'Trained model loss function loaded...\\')\\nprint(f\"Previously trained for {epoch} number of epochs...\")\\n# train for more epochs\\nepoch = 5\\nprint(f\"Train for {epoch} more epochs...\")\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}